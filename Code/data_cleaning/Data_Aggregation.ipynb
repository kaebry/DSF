{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Raw Data\n",
    "this notebook aggregates the all the data we collected and renders csv files for the soil, water, weather and solar radiation data in hourly intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Soil Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv files \n",
    "Saugspannung_2012 = pd.read_csv(\"../../Data/raw_data/all_data_raw/saugspannung 2012.csv\",sep=\",\")\n",
    "Saugspannung_2013 = pd.read_csv(\"../../Data/raw_data/all_data_raw/saugspannung 2013.csv\",sep=\",\")\n",
    "Saugspannung_2014 = pd.read_csv(\"../../Data/raw_data/all_data_raw/saugspannung 2014.csv\",sep=\",\")\n",
    "Saugspannung_2015 = pd.read_csv(\"../../Data/raw_data/all_data_raw/saugspannung 2015.csv\",sep=\",\")\n",
    "Saugspannung_2016 = pd.read_csv(\"../../Data/raw_data/all_data_raw/saugspannung 2016.csv\",sep=\",\")\n",
    "Saugspannung_2017 = pd.read_csv(\"../../Data/raw_data/all_data_raw/saugspannung 2017.csv\",sep=\",\")\n",
    "Saugspannung_2018 = pd.read_csv(\"../../Data/raw_data/all_data_raw/saugspannung 2018.csv\",sep=\",\")\n",
    "Saugspannung_2019 = pd.read_csv(\"../../Data/raw_data/all_data_raw/saugspannung 2019.csv\",sep=\",\")\n",
    "Saugspannung_2020 = pd.read_csv(\"../../Data/raw_data/all_data_raw/saugspannung 2020.csv\",sep=\",\")\n",
    "Saugspannung_2021 = pd.read_csv(\"../../Data/raw_data/all_data_raw/saugspannung 2021.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the csv files from each year to one dataframe\n",
    "saugspannung = pd.concat([Saugspannung_2012,Saugspannung_2013,Saugspannung_2014,Saugspannung_2015,Saugspannung_2016,Saugspannung_2017,Saugspannung_2018,\n",
    "Saugspannung_2019,Saugspannung_2020,Saugspannung_2021])\n",
    "# rename column\n",
    "saugspannung = saugspannung.rename(columns={\"DateTime\": \"datetime\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytz package for handling timezones\n",
    "import pytz\n",
    "# convert DateTime column into datetime format\n",
    "saugspannung[\"datetime\"] = pd.to_datetime(saugspannung['datetime'])\n",
    "# function that converts datetime column into CET timezone and accounts for Daylight saving\n",
    "def add_timezone(df):\n",
    "    timeZone = pytz.timezone(\"CET\")\n",
    "    df['datetime'] = df['datetime'].dt.tz_localize('UTC')\n",
    "    df['datetime'] = df['datetime'].dt.tz_convert(timeZone)\n",
    "    return df\n",
    "\n",
    "add_timezone(saugspannung)\n",
    "\n",
    "# set DateTime as index\n",
    "saugspannung.set_index(\"datetime\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 73849 entries, 2012-01-01 01:00:00+01:00 to 2021-12-31 01:00:00+01:00\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Wädenswil (20cm)  73622 non-null  float64\n",
      " 1   Wädenswil (40cm)  73737 non-null  float64\n",
      " 2   Wädenswil (70cm)  73454 non-null  float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "saugspannung.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe aggregated dataframe as csv file\n",
    "saugspannung.to_csv(\"../../Data/raw_data/saugspannung_agg.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Water Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "path = \"../../Data/raw_data/all_data_raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Obersee\n",
    "############################################################\n",
    "\n",
    "# Merge Datasets for Obersee (Station 2014)\n",
    "Obersee1 = pd.read_csv(path + \"2014_Pegel_Stundenmittel_2008-01-01_2018-12-31.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "Obersee1[\"datetime\"] = pd.to_datetime(Obersee1[\"Zeitstempel\"])\n",
    "Obersee1.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "Obersee2 = pd.read_csv(path + \"2014_Pegel_Stundenmittel_2019-01-01_2022-11-13.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "Obersee2[\"datetime\"] = pd.to_datetime(Obersee2[\"Zeitstempel\"])\n",
    "Obersee2.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "dfs = [Obersee1, Obersee2]\n",
    "obersee = pd.concat(dfs)\n",
    "obersee = obersee.rename(columns={\"Wert\": \"Pegel Obersee\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Limmat\n",
    "############################################################\n",
    "\n",
    "# Merge Datasets for Limmat Abfluss (Station 2099)\n",
    "Lim_Ab1 = pd.read_csv(path + \"2099_Abfluss_Stundenmittel_2000-01-01_2018-12-31.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "Lim_Ab1[\"datetime\"] = pd.to_datetime(Lim_Ab1[\"Zeitstempel\"])\n",
    "Lim_Ab1.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "Lim_Ab2 = pd.read_csv(path + \"2099_Abfluss_Stundenmittel_2019-01-01_2022-11-13.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "Lim_Ab2[\"datetime\"] = pd.to_datetime(Lim_Ab2[\"Zeitstempel\"])\n",
    "Lim_Ab2.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "lim = [Lim_Ab1, Lim_Ab2]\n",
    "lim_abfluss = pd.concat(lim)\n",
    "\n",
    "# Merge Datasets for Limmat Pegel (Station 2099)\n",
    "Lim_peg1 = pd.read_csv(path + \"2099_Pegel_Stundenmittel_2000-01-01_2018-12-31.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "Lim_peg1[\"datetime\"] = pd.to_datetime(Lim_peg1[\"Zeitstempel\"])\n",
    "Lim_peg1.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "Lim_peg2 = pd.read_csv(path + \"2099_Pegel_Stundenmittel_2019-01-01_2022-11-13.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "Lim_peg2[\"datetime\"] = pd.to_datetime(Lim_peg2[\"Zeitstempel\"])\n",
    "Lim_peg2.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "limp = [Lim_peg1, Lim_peg2]\n",
    "lim_pegel = pd.concat(limp)\n",
    "\n",
    "# merge limmat datasets\n",
    "limmat = pd.DataFrame.merge(lim_abfluss, lim_pegel, how=\"left\", on=\"datetime\")\n",
    "limmat = limmat.rename(columns={\"Wert_x\": \"Abfluss Limmat\", \"Wert_y\": \"Pegel Limmat\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Linth\n",
    "############################################################\n",
    "\n",
    "# Merge Datasets for Linth Abfluss (Station 2104)\n",
    "linth_ab1 = pd.read_csv(path + \"2104_Abfluss_Stundenmittel_2000-01-01_2018-12-31.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "linth_ab1[\"datetime\"] = pd.to_datetime(linth_ab1[\"Zeitstempel\"])\n",
    "linth_ab1.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "linth_ab2 = pd.read_csv(path + \"2104_Abfluss_Stundenmittel_2019-01-01_2022-11-13.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "linth_ab2[\"datetime\"] = pd.to_datetime(linth_ab2[\"Zeitstempel\"])\n",
    "linth_ab2.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "lin = [linth_ab1, linth_ab2]\n",
    "linth_abfluss = pd.concat(lin)\n",
    "\n",
    "# Merge Datasets for Linth Pegel (Station 2104)\n",
    "linth_peg1 = pd.read_csv(path + \"2104_Pegel_Stundenmittel_2000-01-01_2018-12-31.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "linth_peg1[\"datetime\"] = pd.to_datetime(linth_peg1[\"Zeitstempel\"])\n",
    "linth_peg1.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "linth_peg2 = pd.read_csv(path + \"2104_Pegel_Stundenmittel_2019-01-01_2022-11-13.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "linth_peg2[\"datetime\"] = pd.to_datetime(linth_peg2[\"Zeitstempel\"])\n",
    "linth_peg2.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "lin1 = [linth_peg1, linth_peg2]\n",
    "linth_pegel = pd.concat(lin1)\n",
    "\n",
    "# Merge Datasets for Linth Wassertemperatur (Station 2104)\n",
    "linth_temp1 = pd.read_csv(path + \"2104_Wassertemperatur_Stundenmittel_2000-01-01_2018-12-31.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "linth_temp1[\"datetime\"] = pd.to_datetime(linth_temp1[\"Zeitstempel\"])\n",
    "linth_temp1.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "linth_temp2 = pd.read_csv(path + \"2104_Wassertemperatur_Stundenmittel_2019-01-01_2022-11-13.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "linth_temp2[\"datetime\"] = pd.to_datetime(linth_temp2[\"Zeitstempel\"])\n",
    "linth_temp2.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "lin2 = [linth_temp1, linth_temp2]\n",
    "linth_temp = pd.concat(lin2)\n",
    "\n",
    "\n",
    "# merge linth datasets\n",
    "linth = pd.merge(pd.merge(linth_abfluss, linth_pegel, how=\"left\", on=\"datetime\"), linth_temp, how=\"left\", on=\"datetime\")\n",
    "linth = linth.rename(columns={\"Wert_x\": \"Abfluss Linth\", \"Wert_y\": \"Pegel Linth\", \"Wert\": \"Wasser_temp Linth\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################################\n",
    "# Zürichsee\n",
    "############################################################\n",
    "\n",
    "\n",
    "# Merge Datasets for Zürichsee (Station 2209)\n",
    "see1 = pd.read_csv(path + \"2209_Pegel_Stundenmittel_2000-01-01_2018-12-31.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "see1[\"datetime\"] = pd.to_datetime(see1[\"Zeitstempel\"])\n",
    "see1.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "see2 = pd.read_csv(path + \"2209_Pegel_Stundenmittel_2019-01-01_2022-11-13.csv\", encoding='latin1', skiprows = 8, sep=\";\").loc[:,[\"Zeitstempel\",\"Wert\"]]\n",
    "see2[\"datetime\"] = pd.to_datetime(see2[\"Zeitstempel\"])\n",
    "see2.drop(columns=\"Zeitstempel\", inplace=True)\n",
    "seez = [see1,see2]\n",
    "zurichsee = pd.concat(seez)\n",
    "zurichsee= zurichsee.rename(columns={\"Wert\": \"Pegel Zurichsee\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all datasets and chagne order of columns\n",
    "wasser_agg = pd.merge(pd.merge(pd.merge(zurichsee, obersee, how=\"left\", on=\"datetime\"), linth, how=\"left\", on=\"datetime\"), limmat, how=\"left\", on=\"datetime\")\n",
    "wasser_agg = wasser_agg.iloc[:,[1,0,2,3,4,5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytz package for handling timezones\n",
    "import pytz\n",
    "# convert DateTime column into datetime format\n",
    "wasser_agg[\"datetime\"] = pd.to_datetime(wasser_agg['datetime'])\n",
    "# function that converts datetime column into CET timezone and accounts for Daylight saving\n",
    "def add_timezone(df):\n",
    "    timeZone = pytz.timezone(\"CET\")\n",
    "    df['datetime'] = df['datetime'].dt.tz_localize('UTC')\n",
    "    df['datetime'] = df['datetime'].dt.tz_convert(timeZone)\n",
    "    return df\n",
    "\n",
    "add_timezone(wasser_agg)\n",
    "\n",
    "# set DateTime as index\n",
    "wasser_agg.set_index(\"datetime\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 200471 entries, 2000-01-01 01:00:00+01:00 to 2022-11-14 00:00:00+01:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   Pegel Zurichsee    200471 non-null  float64\n",
      " 1   Pegel Obersee      130343 non-null  float64\n",
      " 2   Abfluss Linth      200471 non-null  float64\n",
      " 3   Pegel Linth        200471 non-null  float64\n",
      " 4   Wasser_temp Linth  200469 non-null  float64\n",
      " 5   Abfluss Limmat     200470 non-null  float64\n",
      " 6   Pegel Limmat       200470 non-null  float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 12.2 MB\n"
     ]
    }
   ],
   "source": [
    "wasser_agg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe aggregated water data\n",
    "wasser_agg.to_csv(\"../../Data/raw_data/waterdata_agg.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggreagate Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset \n",
    "mythenquai = pd.read_csv(\"../../Data/raw_data/all_data_raw/messwerte_mythenquai_2007-2021 (2).csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime and drop other timestamp columns\n",
    "mythenquai['datetime'] = pd.to_datetime(mythenquai['timestamp_utc'])\n",
    "mythenquai.drop(columns=[\"timestamp_utc\", \"timestamp_cet\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mythenquai_sum = mythenquai[[\"datetime\", \"precipitation\", \"global_radiation\"]]\n",
    "mythenquai_mean = mythenquai.drop(columns=[\"precipitation\", \"global_radiation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert observations into observations with intervals of one hour \n",
    "df_mean = mythenquai_mean.groupby([pd.Grouper(freq='H', key='datetime')]).mean().reset_index()\n",
    "df_sum = mythenquai_sum.groupby([pd.Grouper(freq='H', key='datetime')]).sum().reset_index()\n",
    "# set datetime as index\n",
    "df = pd.merge(df_mean,df_sum, how=\"left\", on=\"datetime\")\n",
    "df.set_index(\"datetime\", inplace=True)\n",
    "# convert datetime index to central european timezone\n",
    "df = df.tz_convert('CET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for change rate of water level\n",
    "df[\"change_rate\"] = df[\"water_level\"].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 128813 entries, 2007-04-22 21:00:00+02:00 to 2022-01-01 00:00:00+01:00\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   air_temperature          126659 non-null  float64\n",
      " 1   water_temperature        109907 non-null  float64\n",
      " 2   wind_gust_max_10min      126659 non-null  float64\n",
      " 3   wind_speed_avg_10min     126659 non-null  float64\n",
      " 4   wind_force_avg_10min     126659 non-null  float64\n",
      " 5   wind_direction           126659 non-null  float64\n",
      " 6   windchill                126659 non-null  float64\n",
      " 7   barometric_pressure_qfe  125870 non-null  float64\n",
      " 8   dew_point                126659 non-null  float64\n",
      " 9   humidity                 126659 non-null  float64\n",
      " 10  water_level              109907 non-null  float64\n",
      " 11  precipitation            128813 non-null  float64\n",
      " 12  global_radiation         128813 non-null  float64\n",
      " 13  change_rate              128812 non-null  float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 14.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../Data/raw_data/mythenquai_agg.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Extraterrestial Solar Radiation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file and rename column\n",
    "RA = pd.read_csv(\"../../Data/raw_data/all_data_raw/solar_radiation_raw.csv\",skiprows=14, sep=\";\")\n",
    "RA= RA.rename(columns={\"# Year\": \"Year\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only date and extraterrestial radiation columns\n",
    "cols = [3,4,6,7]\n",
    "RA.drop(RA.columns[cols], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime variable and drop old columns with Year, Month and Day values\n",
    "RA[\"datetime\"] = pd.to_datetime(RA[['Year', 'Month', 'Day']])\n",
    "RA.drop(RA.columns[[0,1,2]], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytz package for handling timezones\n",
    "import pytz\n",
    "# function that converts datetime column into CET timezone and accounts for Daylight saving\n",
    "def add_timezone(df):\n",
    "    timeZone = pytz.timezone(\"CET\")\n",
    "    df['datetime'] = df['datetime'].dt.tz_localize('UTC')\n",
    "    df['datetime'] = df['datetime'].dt.tz_convert(timeZone)\n",
    "    return df\n",
    "\n",
    "add_timezone(RA)\n",
    "# set datetime variable as index\n",
    "RA.set_index(\"datetime\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2891 entries, 2012-01-01 01:00:00+01:00 to 2019-11-30 01:00:00+01:00\n",
      "Data columns (total 1 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Irradiance (W/m2)  2891 non-null   float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 45.2 KB\n"
     ]
    }
   ],
   "source": [
    "RA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample daily values to hourly values using the resample method and fill hourly NaNs with daily values of the respective day \n",
    "RA = RA.resample('H').ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide all values in the radiation column by 24 to have hourly values \n",
    "RA = RA[[\"Irradiance (W/m2)\"]]/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 69361 entries, 2012-01-01 01:00:00+01:00 to 2019-11-30 01:00:00+01:00\n",
      "Freq: H\n",
      "Data columns (total 1 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Irradiance (W/m2)  69361 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "RA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe dataframe as csv\n",
    "RA.to_csv(\"../../Data/raw_data/extrater_radiation_agg.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dsf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33dbeeb1e0acb46a7865d1331f999416a9ee3844dbf7752d63c1fb0c04927faa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
